{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Языковая модель\n",
    "\n",
    "Наша  цель-построить языковую модель, используя рекуррентную нейронную сеть. Пусть в предложении m слов. Языковая модель позволяет нам предсказывать вероятность данного предложения в виде:\n",
    " \\begin{aligned} P(w_1,...,w_m) = \\prod_{i=1}^{m}P(w_i \\mid w_1,..., w_{i-1}) \\end{aligned} \n",
    "Т.е. вероятность каждого слова предсказывается, исходя из того, какими были предыдущие слова.\n",
    "\n",
    "Рассмотрим возможности этой модели.\n",
    "\n",
    "Во-первых, мы можем использовать её для машинного перевода и распознавания речи: берём наиболее вероятное предложение.\n",
    "\n",
    "Во-вторых, мы можем создавать тексты благодаря возможности выбора последующего слова на основании уже выбранных предыдущих.\n",
    "\n",
    "**NB** В формуле условной вероятности слово $w_i$ зависит от всех предыдущих слов. В теории рекуррентные сети способны использовать \"бесконечную\" память, однако на практике мы столкнёмся с проблемой затухающего градиента. Поэтому количество предшествующих слов, которые будут использоваться для предсказывания следующего слова, следует ограничить.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка текста \n",
    "\n",
    "Аналогичные действия рассматривались на лекции по работе с word2vec. См. https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors\n",
    "\n",
    "Наша задача - создание модели, генерирующей новый текст. Обучающая выборка-текст. Вектор ответов нам не требуется, однако тест нужно представить в виде набора one-hot-encoded векторов.\n",
    "\n",
    "Наши шаги:\n",
    "\n",
    "1.Разделить текст на предложения, а предложения - на слова.\n",
    "\n",
    "2.Составить словарь из 5000 наиболее часто встречаемых слов, остальные слова включить в словарь как  UNKNOWN_TOKEN.\n",
    "\n",
    "3.Добавить в словарь индикаторы начала и конца предложения. Это позволит предсказывать первое слово в предложении.\n",
    "\n",
    "4.Создать индексацию (связку) между представлением слов в виде векторов и их номером в словаре.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 5000 #размер словаря\n",
    "unknown_token = \"UNKNOWN_TOKEN\" # обозначение редких слов в словаре\n",
    "sentence_start_token = \"SENTENCE_START\" #индикатор начала предложения\n",
    "sentence_end_token = \"SENTENCE_END\" #индикатор конца предложения\n",
    " \n",
    "# Загрузите данные из файла reddit-comments-2015-08.csv\n",
    "# Используя nltk.sent_tokenize, разделите каждый текст на предложения (не забудьте перевести всё в нижний регистр)  \n",
    "# NB В случае возникновения проблем примените к тексту decode('utf-8')\n",
    "# К каждому предложению добавьте sentence_start_token и sentence_end_token\n",
    "# Ваш код ...\n",
    "\n",
    "\n",
    "     \n",
    "# Разбейте предложения на слова, используя nltk.word_tokenize\n",
    "# Ваш код ...\n",
    " \n",
    "# Для каждого слова посчитайте его частоту с помощью nltk.FreqDist\n",
    "# Ваш код ...\n",
    "\n",
    "\n",
    " \n",
    "# Создайте словарь, используя 5000 наиболее часто встречающихся слов, используя most_common\n",
    "# Создайте связку index_to_word - просто список слов из словаря\n",
    "# Все остальные слова считаем как unknown_token. Добавьте unknown_token в список index_to_word\n",
    "# Создайте обратную связку word_to_index: словарь(имеется в виду тип данных) пар слово-индекс (чтобы получить такую связку используйте enumerate(index_to_word))\n",
    "# Ваш код ...\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# Замените в полученных предложениях слова, которые не вошли в словарь, на unknown_token\n",
    "# Ваш код ...\n",
    " \n",
    "# Создайте обучающую выборку: для каждого предложения берем все слова, кроме последнего\n",
    "# Вектор ответов: все слова, кроме первого\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "Создадим класс RNN с соответствующими атрибутами данных и методами. Используем Theano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano\n",
    "Theano – библиотека, позволяющая определять, оптимизировать и вычислять математические выражения, особенно удобно использовать Theano для работы с  многомерным массивами. Большое достоинство Theano в том, что с помощью этой библиотеки можно автоматически дифференцировать выражения, используя chain rule. Это возможно благодаря моделированию графа вычислений http://deeplearning.net/software/theano/extending/graphstructures.html\n",
    "\n",
    "Нейронные сети легко представляются как граф вычислений, поэтому библиотека очень удобна для работы с ними.\n",
    " \n",
    "Некоторые сведения о работе с Theano:\n",
    "\n",
    "•\tОпределяем векторы, скаляры, матрицы  следующим образом:\n",
    "\n",
    "a=T.scalar('a')\n",
    "\n",
    "b=T.vector('b')\n",
    "\n",
    "X = T.matrix('X')\n",
    "\n",
    "\n",
    "•\tДля вычисления  заданных выражений используем eval:\n",
    "\n",
    "**(X * 2).eval({X : [[1,1],[2,2]] })**\n",
    "    \n",
    "•\tДля работы с переменными, определенными выше, нужно будет присвоить им значения.\n",
    "В Theano существуют переменные shared с постоянным значением, хранящимся в памяти. \n",
    "\n",
    "**W1 = theano.shared(5, name='W1')**\n",
    "\n",
    "\n",
    "•\tФункции **func = theano.function([x], expression(x))**\n",
    "\n",
    "То есть передаём в function аргумент-x, и то, что должна вывести функция для этого аргумента\n",
    "\n",
    "Пример **func = theano.function([x],x*2)**\n",
    "\n",
    "•\tЕсли предполагается, что функция обновляет значения некоторых параметров, то конструкция следующая\n",
    "\n",
    "**func = theano.function([input_parameters], [], updates=[(old_1, new_1), ..., (old_n,new_n)])**\n",
    "\n",
    "•\tМожно использовать разные функции из theano.tensor: **argmax, grad, T.nnet.categorical_crossentropyropy, tanh, T.nnet.softmax, также sum**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano as theano\n",
    "import theano.tensor as T\n",
    "\n",
    "#word_dim - размерность словаря \n",
    "#hidden_dim=50 - размерность скрытого слоя\n",
    "#bptt_truncate=3 - число предыдущих шагов, учитываемое при запуске механизма обратного распространения ошибки\n",
    "\n",
    "class RNNTheano:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=50, bptt_truncate=3):\n",
    "        # переменным экземпляра класса назначаются значения, передаваемые пользователем \n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        # Матрицы V,W  задайте случайными значениями из интервала (-1/sqrt(hidden_dim))\n",
    "        # Матрицу U  задайте случайными значениями из интервала (-1/sqrt(word_dim))\n",
    "        # Ваш код... \n",
    "        \n",
    "        \n",
    "        # Создайте shared variables U,V,W: \n",
    "        self.U = theano.shared(name='U', value=U.astype(theano.config.floatX))\n",
    "\n",
    "        \n",
    "        # Для использования chain rule в вычислении градиента нам понадобится граф вычислений. Храним его здесь\n",
    "        self.theano = {}\n",
    "        self.__theano_build__()\n",
    "    \n",
    "    def __theano_build__(self):\n",
    "        U, V, W = self.U, self.V, self.W\n",
    "        x = T.ivector('x')\n",
    "        y = T.ivector('y')\n",
    "        # вычисляем внутреннее состояние s и выход o на шаге t (формулы - см.лекции)\n",
    "        def forward_prop_step(x_t, s_t_prev, U, V, W): #s_t_prev - внутреннее состояние на предыдущем шаге\n",
    "            s_t = \n",
    "            o_t = \n",
    "            return \n",
    "        # Вычисление o и s в цикле http://deeplearning.net/software/theano/library/scan.html\n",
    "        [o,s], updates = theano.scan(\n",
    "            forward_prop_step,\n",
    "            sequences=x,\n",
    "            outputs_info=[None, dict(initial=T.zeros(self.hidden_dim))],\n",
    "            non_sequences=[U, V, W],\n",
    "            np.random.multinomial(1, next_word_probs[-1])=self.bptt_truncate,\n",
    "            strict=True)\n",
    "        \n",
    "        #вычисляем слово, имеющее наибольшую вероятность o\n",
    "        prediction = \n",
    "        #вычисляем кросс-энтропию\n",
    "        o_error = \n",
    "        \n",
    "        \n",
    "        # вычисляем градиенты o_error по U,V,W\n",
    "        dU = \n",
    "        dV = \n",
    "        dW = \n",
    "        \n",
    "        # создаём функции, как это принято в Theano\n",
    "        self.forward_propagation =                      #записываем в ответ только o (s не записываем)\n",
    "        self.predict = \n",
    "        self.ce_error = \n",
    "        self.bptt =  #зависит от x,y и возвращает dU,dV,dW\n",
    "        \n",
    "        # Градиентный спуск\n",
    "        # определите скалярную величину learning_rate, как это приянято в Theano\n",
    "        learning_rate = \n",
    "        # матрицы U,V,W обновляем, двигаясь в направлении антиградиента\n",
    "        # запишите функцию градиентного шага с updates\n",
    "        self.sgd_step = theano.function([x,y,learning_rate], [], \n",
    "                      updates=   \n",
    "                                       )\n",
    "    #вычисляем ошибку - суммируем ce_error по всем объектам X,Y\n",
    "    def calculate_total_loss(self, X, Y):\n",
    "        return \n",
    "    \n",
    "    #разделим calculate_total_loss на число слов\n",
    "    def calculate_loss(self, X, Y):\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=1, evaluate_loss_after=5):\n",
    "    # для визуализации ошибки сохраняем её значение на каждой пятой итерации\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # если номер интерации кратен 5:вычисляем значение calculate_loss, \n",
    "        #                               записываем кортеж (число просмотренных объектов,ошибка) в список loss, \n",
    "        #                               если ошибка на последней выполненной итерации больше, чем на предпоследней, то уменьшаем learning_rate в 2 раза\n",
    "        \n",
    "            \n",
    "            \n",
    "        # для каждого объекта из выборки делаем шаг градиентного спуска и увеличиваем на 1 кол-во просмотренных объектов num_examples_seen\n",
    "        for i in range(len(y_train)):\n",
    "           \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование модели для генерации текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sentence(model):\n",
    "    # новое предложение представляем в виде списка, добавляем в него элемент sentence_start_token из  word_to_index\n",
    "    new_sentence = \n",
    "    # пока не встречаем sentence_end_token:\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs =  #вычисляем для каждого слова словаря вероятность быть следующим в нашем предложении\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        # нам не нужен выбор слова,которое не попало в наш словарь\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            # на каждом шаге t получаем на выходе вектор o вероятностей для каждого слова из слоаваря, \n",
    "            # нас интересует o на последнем шаге\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1]) #сэмплируем индес (получаем строку из 0 и 1)\n",
    "            sampled_word = np.argmax(samples) #берём индекс, указывающий на 1\n",
    "        new_sentence.append(sampled_word)# это индекс\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    "\n",
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # хотим предложения подлиннее\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print \" \".join(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрите, какое время занимает один градиентный шаг для объекта класса RNNTheano с 50 нейронами скрытого слоя и 5000 слов в словаре"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели даже с такими небольшими значениями параметров (vocabularu_size=5000, hidden_dim=50, bptt_truncate=3, nepoch=50) займёт много времени"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у вас нет возможности обучить модель на всей выборке, обучите её на 100 объектах с nepoch=10 и постройте график зависимости ошибки от номера итерации. Если всё сделано верно, ошибка должна уменьшаться"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
